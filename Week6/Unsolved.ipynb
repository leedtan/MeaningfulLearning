{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This Section (Spanning two weeks) Will cover neural networks.\n",
    "# This is the final math step in putting together all of the stuff we have learned about linear\n",
    "# Regression, math, and python. After this, we can start doing more intelligent ML."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Below, we generate the Xor dataset\n",
    "import numpy as np\n",
    "\n",
    "n_data = 1000\n",
    "x = np.random.randint(2, size=(n_data, 2))\n",
    "y = x[:,0] != x[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's train liear regression to predict the y values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mdl = # TODO train a Ridge regression model on x, y\n",
    "\n",
    "mdl.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# And let's check the accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat = # TODO Calculate the yhat predictions (is the prediction greater than .5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(yhat == y).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What the heck! Why is our accuracy 50%? That's not very good. Let's look below at the problem we're\n",
    "# Solving. How could we solve that? Should x1 and x2 have positive or negative coeficients? \n",
    "# There isn't really an answer, so the model just sets them to close to 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "Image(filename='xor.png', width=200, height=200) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What if you had features \n",
    "# x1, x2, x3=(x1 OR x2) \n",
    "# (essentially, x3 is only equal to one at the top right corner of the above image)?\n",
    "# Would you be able to learn linear regression to solve this problem? \n",
    "# What you you assign the weights to be?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert(False)\n",
    "# TODO: Pick features that would perfectly solve this problem, then remove the assertion error above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x3 = # TODO: Calculate x3 = (x1 or x2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_expanded = # TODO: make a single x that contains x and x3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mdl_improved = Ridge().fit(x_expanded, y)\n",
    "mdl_improved.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Oh! Check it out, the model learned exactly what we wanted it to learn! (Well, almost)\n",
    "# What if we reduced our regularization, do we think the model would get closer to [1, 1, -2],\n",
    "# Or further away?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Guess if the model's coefficients will be closer to [1, 1, -2] or further away\n",
    "mdl_improved = Ridge(0).fit(x_expanded, y)\n",
    "mdl_improved.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wow! It's perfect! Great.\n",
    "# So the problem here is, we already knew to compute x3, that new feature\n",
    "# Before we improved our model.\n",
    "# Let's do a few things with our original x, y dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, let's check if other models could solve this problem.\n",
    "# Let's try a decision tree, and a neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = # TODO Train a model based on a DecisionTreeRegressor\n",
    "yhat_dt = dt.predict(x)\n",
    "(yhat_dt == y).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPRegressor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn = # TODO Train a model based on a MLPRegressor (use hidden shapes (100, 100))\n",
    "yhat_nn = nn.predict(x) > .5\n",
    "(yhat_nn == y).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Okay, so we know that it's possible for a model to automatically learn stuff like our \n",
    "# \"x_expanded\" from earlier, that they can use to solve x_or\n",
    "# So let's try to implement a model like linear regression, but with this quirk.\n",
    "# Let's try a bunch of linear regressions each computing their own features\n",
    "# Let's call teh number of linear regressions we do in this first layer num_hidden,\n",
    "# Because we call this the hidden layer of our neural network.\n",
    "# We can treat these\n",
    "# Three like features from a fake dataset, and use them to do linear regression to predict y."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's use 100 hidden units. Feel free to change this\n",
    "num_hidden = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_lr = # TODO Initialize the first linear regression models (weight matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "second_lr = # TODO Initialize the first second regression models (weight matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# So here's the issue. If we do this directly, it will mathematically represent the same thing\n",
    "# As a single linear regression from x prediction y.\n",
    "# Here are some links that describe the issue. I won't go into it in detail\n",
    "# https://math.stackexchange.com/questions/1948502/show-that-multiplication-of-matrices-corresponds-to-composition-of-linear-transf\n",
    "# http://www.math.lsa.umich.edu/~kesmith/217worksheet2-3ALT1.pdf\n",
    "# Let me propose a specific trick to get around this though. All we need is to not compose linear \n",
    "# Functions. So how about we look at the features we computed with the first step, let's put them\n",
    "# Through this kink function (called a leaky relu), that is NOT LINEAR.\n",
    "# Now we're good to go"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def leaky_relu(x):\n",
    "    return # TODO calculate the  output of a leaky relu, which is equal to x, unless x is negative, then is .1 * x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xviz = np.linspace(-10, 10, 1000)\n",
    "\n",
    "yviz = leaky_relu(xviz)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(xviz, yviz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Great! So now let's build it.\n",
    "features = # TODO calculate the hidden layer feature values. These will be used to run the final linear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat = # TODO Calculate the output y prediction vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = # TODO Calculate our loss\n",
    "dl_dyhat = # TODO Calculate the derivitive from the loss to yhat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dl_dyhat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "second_lr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dl_dsecond_lr = # TODO Calculate the derivitive\n",
    "dl_dfeatures = # TODO Calculate the derivitive "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dl_dfeatures.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the features of each of our derivitives! For instance, dl_dfeatures is the same shape \n",
    "# as features. This makes sense, because the derivitive tells us which direction it wants us to change\n",
    "# Each of the values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dl_dfeatures.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dl_dfirst_lr = # TODO Calculate the derivitive from the loss to yhat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Okay! We calculated all our derivitives. Let's put this in a for loop to update our parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "lr = .0001\n",
    "first_lr = np.random.rand(2, num_hidden)/10000\n",
    "second_lr = np.random.rand(num_hidden)/10000\n",
    "for update_step in range(100):\n",
    "    features = # TODO \n",
    "    yhat = # TODO \n",
    "    loss = # TODO \n",
    "    dl_dyhat = # TODO \n",
    "    dl_dsecond_lr = # TODO \n",
    "    dl_dfeatures = # TODO \n",
    "    dl_dfirst_lr = # TODO \n",
    "    first_lr = # TODO Calculate the derivitive from the loss to yhat\n",
    "    second_lr = # TODO Calculate the derivitive from the loss to yhat\n",
    "    if update_step % 10 == 0:\n",
    "        print(update_step, 'loss', loss)\n",
    "        print(update_step, 'acc', ((yhat > 0.5) == y).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Okay, so we were able to train the model and reduce the loss, but we still didn't\n",
    "# learn accuracy greater than half.\n",
    "# So let's do that thing we talked about earlier, and put the leaky relu in for our \"hidden features\"\n",
    "# (the one's after the first linear regression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "lr = .00001\n",
    "first_lr = np.random.rand(2, num_hidden)/10000\n",
    "second_lr = np.random.rand(num_hidden)/10000\n",
    "for update_step in range(100):\n",
    "    features = # TODO \n",
    "    features = # TODO: Apply the leaky relu\n",
    "    yhat = # TODO \n",
    "    loss = # TODO \n",
    "    dl_dyhat = # TODO \n",
    "    dl_dsecond_lr = # TODO \n",
    "    dl_dfeatures = # TODO \n",
    "    dl_dfeatures = # TODO Added. How we calculate the derivitive to the values before the leaky relu?\n",
    "    dl_dfirst_lr = # TODO \n",
    "    first_lr = # TODO \n",
    "    second_lr = # TODO \n",
    "    if update_step % 10 == 0:\n",
    "        print(update_step, 'loss', loss)\n",
    "        print(update_step, 'acc', ((yhat > 0.5) == y).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WOOO! We got 100% accuracy! That's called a neurel network.\n",
    "# We can also insert more hidden layers. Each hidden layer's job is just to compute teh next\n",
    "# Hidden layer, before finally the last one is used to perform the final linear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
